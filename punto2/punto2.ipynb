{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('datos_observacionales.txt')\n",
    "sig=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "#from mpl_toolkits.mplot3d import Axes3d\n",
    "\n",
    "#condiciones inciales dadas por data[0,1:]\n",
    "#metodo de euler y+1=y0+hf(x,y)\n",
    "\n",
    "#obs[0] = x asociado a sigma , obs[1]=y asociado a sigma rho obs[2]=z asociado a beta\n",
    "\n",
    "#define el tamaño de cada paso de la gradiente\n",
    "step = 0.004\n",
    "delta = 0.1 #tamaño del paso en el leapfrog\n",
    "m=100.0\n",
    "\n",
    "step_sig=10\n",
    "step_rh=10\n",
    "step_bet=10\n",
    "\n",
    "def funx(sigma,obs):\n",
    "    return sigma*(obs[1]-obs[0])\n",
    "\n",
    "def funy(rh,obs):\n",
    "    return obs[0]*(rh-obs[2])-obs[1]\n",
    "\n",
    "def funz(bet,obs):\n",
    "    return obs[0]*obs[1]-bet*obs[2]\n",
    "\n",
    "#calcula el loglike, llama a modelo. Se asume una likelihood en forma de multiplicatoria-->log es una suma\n",
    "#sig = sigma de los datos (error bars)\n",
    "def loglike(derv,obs,param):\n",
    "    #pram es el parametro a probar\n",
    "    #obs son x y o lo que sea observados\n",
    "    #es las derivada de la coordenana requerida por model\n",
    "    modelo=np.asarray([funx(param[0],obs),funy(param[1],obs),funz(param[2],obs)])\n",
    "    t = (derv-modelo)\n",
    "    t=t/sig\n",
    "    return -0.5*np.sum(t**2)\n",
    "\n",
    "#saca el hamiltoniano\n",
    "def H(p,derv,obs,param):\n",
    "    m=100.0\n",
    "    #model es la funcion que vamos a probar para cada param\n",
    "    K = 0.5 * np.sum(p**2)/m\n",
    "    #la distribucion que queremos samplear es la energia potencias\n",
    "    #la posterior del modelo y el prior van aca\n",
    "    U = -loglike(derv,obs, param)\n",
    "    return K + U\n",
    "\n",
    "\n",
    "#calcula la derivada respecto a cada parametro\n",
    "def grad_loglike(derv,obs,param):\n",
    "    #derv es un array con las derivadas en cada direccion\n",
    "    ret_grad = np.zeros(3)\n",
    "    #movemos sigma\n",
    "    param_sig=param\n",
    "    param_sig[0]=param_sig[0]+step\n",
    "    #movemos rho\n",
    "    param_rho=param\n",
    "    param_rho[0]=param_rho[0]+step\n",
    "    #movemos el beta\n",
    "    param_beta=param\n",
    "    param_beta[0]=param_beta[0]+step\n",
    "    #el step me avanza al siguiente parametro como la definicion de derivada\n",
    "    ret_grad[0]= (loglike(derv[0],obs, param_sig) - loglike(derv[0],obs, param))/step\n",
    "    ret_grad[1] = (loglike(derv[1],obs, param_rho) - loglike(derv[1],obs, param))/step\n",
    "    ret_grad[2] = (loglike(derv[2],obs, param_beta) - loglike(derv[2],obs, param))/step\n",
    "\n",
    "    return ret_grad\n",
    "            \n",
    "#genera el nuevo punto por leapfrog hay un momento asociado a cada uno de los parametros\n",
    "def leapfrog(derv,obs,param, p_in):\n",
    "    #paramson los parametros\n",
    "    #delta es el step del leapfrog\n",
    "    #sig es el error que se le manda a la loglikelihood\n",
    "    p_new = p_in \n",
    "    sig_new =np.copy(param[0])\n",
    "    rh_new =np.copy(param[1])\n",
    "    bet_new =np.copy(param[2])\n",
    "        \n",
    "    n_iter = 5\n",
    "\n",
    "    \n",
    "    for i in range(1,n_iter):\n",
    "        #primer half-step\n",
    "        p_new = p_in + 0.5 * delta * grad_loglike(derv,obs,param)  \n",
    "        #para sacar el nuevo parametro, tenemos que actualizar con la masa param * m=p_param\n",
    "        sig_new = sig_new + delta * p_new[0]/m\n",
    "        rh_new = rh_new + delta *p_new[1]/m\n",
    "        bet_new = bet_new + delta *p_new[2]/m\n",
    "        #actualiza el momento con los nuevos parametros(el segundo half-step)\n",
    "        p_new = p_new + 0.5 * delta * grad_loglike(derv,obs,param)\n",
    "        ######\n",
    "        #no olvidar el -1\n",
    "        #####\n",
    "        p_new = - p_new\n",
    "\n",
    "    param_new=np.asarray([sig_new,rh_new,bet_new])\n",
    "    \n",
    "    return p_new, param_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimado(n_steps):\n",
    "    sigma_param=0.4\n",
    "    parametros=np.zeros((n_steps,3))\n",
    "    p= np.zeros((n_steps,3))\n",
    "    #primer elemento de mi array de logposterior\n",
    "    #log_post=[loglike(x,z,sigma_z,a_e[0],b_e[0],c_e[0])]\n",
    "    parametros[0,:]=np.random.normal(loc=0.0,scale=sigma_param,size=3)\n",
    "    p[0]=np.random.normal(loc=0.0,scale=sigma_param,size=3)\n",
    "   \n",
    "    for i in range(1,n_steps):\n",
    "        obs_old = data[i-1,1:]\n",
    "        obs_new = data[i,1:]\n",
    "        derivates = (obs_new-obs_old)/(data[i,0]-data[i-1,0])\n",
    "        #generamos una propuesta por medio de leapfrog en vez de usar un numero aleatorio\n",
    "        p_new, parametros_new = leapfrog(derivates,obs_new, parametros[i-1,:], p[i-1])\n",
    "        #aca estamos sampleando la distribucion de energia con el hamiltoniano \n",
    "                #p,derv,obs,model,param\n",
    "        E_new = H( p_new,derivates,obs_new,parametros_new) \n",
    "        E_old = H( p[i-1,:],derivates,obs_old, parametros[i-1,:])\n",
    "        \n",
    "        #Aca se aplica la condicion metropolis\n",
    "         #recordemos que nuestra distribucion de proba es la distribucion de gibbs\n",
    "        alpha = min(1,np.exp( - (E_new - E_old))) # Se comparan las dos energias\n",
    "        \n",
    "        beta = np.random.random()\n",
    "         #acepta\n",
    "        if(beta<alpha):\n",
    "            parametros[i,:]=parametros_new\n",
    "        #rechaza\n",
    "        else:\n",
    "            parametros[i,:]=parametros[i-1,:]\n",
    "\n",
    "        #reiniciar el momento\n",
    "        p[i]=(np.random.normal(size=3))\n",
    "    \n",
    "    return parametros\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/lib/python3.6/site-packages/ipykernel_launcher.py:23: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "parametros=estimado(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
